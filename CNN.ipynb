{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) For Detecting Defective Parts\n",
    "\n",
    "This notebook uses the [`TensorFlow`](https://www.tensorflow.org/) library to implement a Convolutional Neural Network (CNN) classifier with various hyperparameters. The CNN classifier is used to detect defective mechanical parts in a manufacturing process. The image data we used is from the [\"Casting Product Image Data For Quality Inspection\"](https://www.kaggle.com/ravirajsinh45/real-life-industrial-dataset-of-casting-product) dataset, and contains 8646 images used for quality inspection that we used to train the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = './casting_512x512/casting_512x512/'\n",
    "OUTPUT_PATH = './out/'\n",
    "TRAIN_TEST_SPLIT = 0.8\n",
    "IMG_PX_SIZE = 128\n",
    "RESIZE_SHAPE = (IMG_PX_SIZE, IMG_PX_SIZE)\n",
    "INPUT_SHAPE = (IMG_PX_SIZE, IMG_PX_SIZE, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods for loading image data, flattening the images, shuffling them, and splitting them into training and test sets. (Credit to Sam Little)\n",
    "\n",
    "Because CNN's and neural networks in general benefit from larger datasets than SVM's, each image is duplicated and flipped about the vertical axis to double the size of training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_image_data():\n",
    "  data = []\n",
    "  labels = []\n",
    "  print('Loading OK images...')\n",
    "  for filename in tqdm(os.listdir(f'{DATA_PATH}/ok_front')):\n",
    "    img = Image.open(f'{DATA_PATH}/ok_front/{filename}')\n",
    "    img_f = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    data.append(img)\n",
    "    labels.append(0)\n",
    "    data.append(img_f)\n",
    "    labels.append(0)\n",
    "  print('Loading defective images...')\n",
    "  for filename in tqdm(os.listdir(f'{DATA_PATH}/def_front')):\n",
    "    img = Image.open(f'{DATA_PATH}/def_front/{filename}')\n",
    "    img_f = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "    data.append(img)\n",
    "    labels.append(1)\n",
    "    data.append(img_f)\n",
    "    labels.append(1)\n",
    "  return data, labels\n",
    "\n",
    "def resize_images(data, shape):\n",
    "  resized_data = []\n",
    "  print(f'Resizing images to {shape}...')\n",
    "  for img in tqdm(data):\n",
    "    resized_data.append(img.resize(shape))\n",
    "  return resized_data\n",
    "\n",
    "def prepare_image_data(data, labels):\n",
    "  images = []\n",
    "  print('Preparing images...')\n",
    "  for img in tqdm(data):\n",
    "    img = np.array(img)\n",
    "    img = img / 255.0\n",
    "    img[img == 0] = 0.0001\n",
    "    images.append(img)\n",
    "  data = np.array(images)\n",
    "  original_shape = data[0].shape\n",
    "  #data = data.reshape(data.shape[0], -1)\n",
    "  shuffled = list(zip(data, labels))\n",
    "\n",
    "  np.random.shuffle(shuffled)\n",
    "  train = shuffled[:int(len(shuffled) * TRAIN_TEST_SPLIT)]\n",
    "  test = shuffled[int(len(shuffled) * TRAIN_TEST_SPLIT):]\n",
    "\n",
    "  train_data = np.array([i[0] for i in train])\n",
    "  train_labels = np.array([i[1] for i in train])\n",
    "  test_data = np.array([i[0] for i in test])\n",
    "  test_labels = np.array([i[1] for i in test])\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels, original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare hyperparameters we wish to test, and feed each into an `TensorFlow` CNN, fitting it to the training data. Accuracy, confusion matrices, and other metrics are generated for each combination.\n",
    "\n",
    "All images are initally resized to 128x128 for the sake of computational speed, and the two convolutional layers further reduce them to 32x32, before being fed through the two densly connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data, labels = load_image_data()\n",
    "data = resize_images(data, RESIZE_SHAPE)\n",
    "train_data, train_labels, test_data, test_labels, original_shape = prepare_image_data(data, labels)\n",
    "\n",
    "optimizers_to_test = ['rmsprop', 'sgd', 'adam', 'adadelta']\n",
    "activations_to_test = ['relu', 'sigmoid']\n",
    "loss_func_to_test = ['sparse_categorical_crossentropy', 'mse', 'categorical_hinge']\n",
    "layer1_sizes_to_test = [128, 256, 512]\n",
    "layer2_sizes_to_test = [16, 32, 64]\n",
    "\n",
    "tests_to_run = []\n",
    "\n",
    "for loss in loss_func_to_test:\n",
    "  for optimizer in optimizers_to_test:\n",
    "    for activation in activations_to_test:\n",
    "      for layer1 in layer1_sizes_to_test:\n",
    "        for layer2 in layer2_sizes_to_test:\n",
    "          tests_to_run.append({\n",
    "            'name': f'{loss}_{optimizer}_{activation}_{layer1}_{layer2}',\n",
    "            'loss': loss,\n",
    "            'optimizer': optimizer,\n",
    "            'activation': activation,\n",
    "            'layer1': layer1,\n",
    "            'layer2': layer2,\n",
    "          })\n",
    "          \n",
    "results = [] \n",
    "           \n",
    "for idx, test in enumerate(tests_to_run):\n",
    "  \n",
    "  print('\\n' + 'Test ', idx+1, '/', len(tests_to_run), ' - ', test['name'])\n",
    "  \n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=INPUT_SHAPE),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(test['layer1'], activation=test['activation']),\n",
    "    tf.keras.layers.Dense(test['layer2'], activation=test['activation']),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "  model.compile(optimizer=test['optimizer'],\n",
    "                loss=test['loss'],\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model.fit(train_data, train_labels, epochs=10)\n",
    "\n",
    "  test_loss, test_acc = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "  predictions = np.argmax(model.predict(test_data), axis=1)\n",
    "  accuracy = accuracy_score(test_labels, predictions)\n",
    "  conf_matrix = confusion_matrix(test_labels, predictions)\n",
    "  report = classification_report(test_labels, predictions)\n",
    "  results.append({\n",
    "    'name': test['name'],\n",
    "    'loss': test['loss'],\n",
    "    'optimizer': test['optimizer'],\n",
    "    'activation': test['activation'],\n",
    "    'layer1': test['layer1'],\n",
    "    'layer2': test['layer2'],\n",
    "    'accuracy': accuracy,\n",
    "    'confusion_matrix': conf_matrix.tolist(),\n",
    "    'classification_report': report,\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each parameter combination, we save the confusion matrix, accuracy, and other metrics to the output directory. (Credit to Sam Little)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for result in results:\n",
    "  result_path = f'{OUTPUT_PATH}/{result[\"loss\"]}/{result[\"optimizer\"]}/{result[\"activation\"]}'\n",
    "  # if path doesn't exist, create it\n",
    "  if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "\n",
    "  # save confusion matrix\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.imshow(result['confusion_matrix'], interpolation='nearest', cmap=plt.cm.Blues)\n",
    "  plt.title(f'Confusion Matrix for {result[\"name\"]}\\nAccuracy: {result[\"accuracy\"] * 100:.2f}%\\nImage Size: {RESIZE_SHAPE}')\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(2)\n",
    "  plt.xticks(tick_marks, ['OK', 'DEFECTIVE'], rotation=45)\n",
    "  plt.yticks(tick_marks, ['OK', 'DEFECTIVE'])\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel('True Label')\n",
    "  plt.xlabel('Predicted Label')\n",
    "  plt.savefig(f'{result_path}/confusion_matrix.png')\n",
    "\n",
    "  # save results to JSON\n",
    "  with open(f'{result_path}/{result[\"name\"]}.json', 'w') as f:\n",
    "    result = {\n",
    "      'name': result['name'],\n",
    "      'loss': result['loss'],\n",
    "      'optimizer': result['optimizer'],\n",
    "      'activation': result['activation'],\n",
    "      'layer1': test['layer1'],\n",
    "      'layer2': test['layer2'],\n",
    "      'accuracy': result['accuracy'],\n",
    "      'confusion_matrix': result['confusion_matrix'],\n",
    "      'confusion_matrix_image': f'{result_path}/confusion_matrix.png',\n",
    "      'classification_report': result['classification_report']\n",
    "    }\n",
    "    json.dump(result, f)\n",
    "    \n",
    "    \n",
    "results.sort(key=lambda x: x['accuracy'], reverse=True)\n",
    "print(\"Best results:\")\n",
    "for index, result in enumerate(results):\n",
    "  print(f'{index + 1}. {result[\"name\"]} - {result[\"accuracy\"] * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbc9ec0222e3d4762a02d5c99a77240335f6993003a5a3482c0d463aab827d93"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
